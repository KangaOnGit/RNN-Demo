{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10252667,"sourceType":"datasetVersion","datasetId":6341830}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unidecode","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:06:55.251786Z","iopub.execute_input":"2024-12-20T08:06:55.252143Z","iopub.status.idle":"2024-12-20T08:06:55.261675Z","shell.execute_reply.started":"2024-12-20T08:06:55.252114Z","shell.execute_reply":"2024-12-20T08:06:55.260647Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nimport torch .nn as nn\n\nseed = 1\ntorch.manual_seed ( seed )\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nimport unidecode\n\nnltk.download ('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nfrom torch.utils.data import Dataset , DataLoader\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:06:39.424334Z","iopub.execute_input":"2024-12-20T08:06:39.424679Z","iopub.status.idle":"2024-12-20T08:06:45.841385Z","shell.execute_reply.started":"2024-12-20T08:06:39.424649Z","shell.execute_reply":"2024-12-20T08:06:45.840207Z"}},"outputs":[{"name":"stdout","text":"Collecting unidecode\n  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\nDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: unidecode\nSuccessfully installed unidecode-1.3.8\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"dataset_path = '/kaggle/input/datasent/dataset/all-data.csv'\nheaders = ['sentiment', 'content']\ndf = pd.read_csv(dataset_path, names=headers, encoding='ISO-8859-1')\n\n\nclasses = {class_name: idx for idx, class_name in enumerate(df['sentiment'].unique().tolist())}\ndf['sentiment'] = df['sentiment'].apply(lambda x: classes[x])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:08:21.502980Z","iopub.execute_input":"2024-12-20T08:08:21.503407Z","iopub.status.idle":"2024-12-20T08:08:21.528398Z","shell.execute_reply.started":"2024-12-20T08:08:21.503365Z","shell.execute_reply":"2024-12-20T08:08:21.527148Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"english_stop_words = stopwords.words('english')\nstemmer = PorterStemmer()\n\ndef text_normalize(text):\n    text = text.lower()\n    text = unidecode.unidecode(text)\n    text = text.strip()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = ' '.join([word for word in text.split(' ') if word not in english_stop_words])\n    text = ' '.join([stemmer.stem(word) for word in text.split(' ')])\n    return text\n\ndf['content'] = df['content'].apply(lambda x: text_normalize(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T08:08:17.410240Z","iopub.execute_input":"2024-12-20T08:08:17.410622Z","iopub.status.idle":"2024-12-20T08:08:18.956992Z","shell.execute_reply.started":"2024-12-20T08:08:17.410589Z","shell.execute_reply":"2024-12-20T08:08:18.955904Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"vocab = []\nfor sentence in df['content'].tolist():\n    tokens = sentence.split()\n    for token in tokens:\n        if token not in vocab:\n            vocab.append(token)\n\nvocab.append('UNK')\nvocab.append('PAD')\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}\nvocab_size = len(vocab)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transform(text, word_to_idx, max_seq_len):\n    tokens = []\n    for w in text.split():\n        try:\n            w_ids = word_to_idx[w]\n        except:\n            w_ids = word_to_idx['UNK']\n        tokens.append(w_ids)\n\n    if len(tokens) < max_seq_len:\n        tokens += [word_to_idx['PAD']] * (max_seq_len - len(tokens))\n    elif len(tokens) > max_seq_len:\n        tokens = tokens[:max_seq_len]\n\n    return tokens","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_size = 0.2\ntest_size = 0.125\nis_shuffle = True\n\ntexts = df['content'].tolist()\nlabels = df['sentiment'].tolist()\n\nX_train, X_val, y_train, y_val = train_test_split(\n    texts, labels, test_size=val_size, random_state=1, shuffle=is_shuffle\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_train, y_train, test_size=test_size, random_state=1, shuffle=is_shuffle\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FinancialNews(Dataset):\n    def __init__(self, X, y, word_to_idx, max_seq_len, transform=None):\n        self.texts = X\n        self.labels = y\n        self.word_to_idx = word_to_idx\n        self.max_seq_len = max_seq_len\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        if self.transform:\n            text = self.transform(text, self.word_to_idx, self.max_seq_len)\n\n        text = torch.tensor(text)\n        return text, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_seq_len = 32\n\ntrain_dataset = FinancialNews(\n    X_train, y_train, word_to_idx=word_to_idx, max_seq_len=max_seq_len, transform=transform\n)\n\nval_dataset = FinancialNews(\n    X_val, y_val, word_to_idx=word_to_idx, max_seq_len=max_seq_len, transform=transform\n)\n\ntest_dataset = FinancialNews(\n    X_test, y_test, word_to_idx=word_to_idx, max_seq_len=max_seq_len, transform=transform\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, n_classes, dropout_prob):\n        super(SentimentClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, hidden_size, n_layers, batch_first=True)\n        self.norm = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc1 = nn.Linear(hidden_size, 16)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(16, n_classes)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x, hn = self.rnn(x)\n        x = x[:, -1, :]\n        x = self.norm(x)\n        x = self.dropout(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_classes = len(classes)\nembedding_dim = 64\nhidden_size = 64\nn_layers = 2\ndropout_prob = 0.2\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = SentimentClassifier(\n    vocab_size=vocab_size,\n    embedding_dim=embedding_dim,\n    hidden_size=hidden_size,\n    n_layers=n_layers,\n    n_classes=n_classes,\n    dropout_prob=dropout_prob\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fit(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n    train_losses, val_losses = [], []\n\n    for epoch in range(epochs):\n        model.train()\n        batch_train_losses = []\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            batch_train_losses.append(loss.item())\n\n        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n        train_losses.append(train_loss)\n\n        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n        val_losses.append(val_loss)\n\n        print(f'Epoch {epoch + 1}: Train loss={train_loss:.4f}, Val loss={val_loss:.4f}')\n\n    return train_losses, val_losses\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_acc, total_loss = 0, 0\n\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            total_acc += (outputs.argmax(1) == labels).sum().item()\n\n    return total_loss / len(dataloader), total_acc / len(dataloader.dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 50\ntrain_losses, val_losses = fit(model, train_loader, val_loader, criterion, optimizer, device, epochs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loss, val_acc = evaluate(model, val_loader, criterion, device)\ntest_loss, test_acc = evaluate(model, test_loader, criterion, device)\n\nprint(f'Val Accuracy: {val_acc:.4f}, Test Accuracy: {test_acc:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}